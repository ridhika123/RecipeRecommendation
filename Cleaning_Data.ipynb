{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26c7947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import time \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7882eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2abf6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c606c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\12242\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\12242\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68408bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# vocabulary = nltk.FreqDist()\n",
    "\n",
    "# # This was done once I had already preprocessed the ingredients\n",
    "# for ingredients in df['ingredients_parsed']:\n",
    "#     str1 = \"\"\n",
    "# #     print(ingredients)\n",
    "#     for i in ingredients:\n",
    "#         str1 += \" \"+i \n",
    "#     ingredients = str1.split()\n",
    "#     vocabulary.update(ingredients)\n",
    "    \n",
    "# for word, frequency in vocabulary.most_common(1000):\n",
    "#     print(f'{word};{frequency}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d500afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary['?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6280cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove everything past a comma\n",
    "# remove everything past a number\n",
    "# remove everything past \"to\", \"for\", \"cut\", \"with\"\n",
    "# remove \"as\", \"required\", \"chopped\", \"medium\", \"fresh\", \"boiled\", \"peeled\", \"and\", \"finely\", \"a\", \"pinch\", \"slice\", \"crushed\", \"fillets\", \"sliced\", \"few\", \"strands\", \"browned\", \"loaf\", \"pieces\", \"into\", \"cubes\", \"garnishing\", \"tablespo\", \"drop\", \"shaving\", \"drained\" \n",
    "# remove anything with \"salt\", \"oil\", \"butter\"\n",
    "# separate as ingredient what is inside () or after /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da862755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import ast\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "def ingredient_cleaner(ingreds):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_to_remove = [\"as\", \"required\", \"chopped\", \"medium\", \"fresh\", \"boiled\", \"peeled\", \"and\", \"finely\", \"a\", \"pinch\",                    \n",
    "                       \"slice\", \"crushed\", \"fillet\", \"sliced\", \"few\", \"strand\", \"browned\", \"loaf\", \"piece\", \"into\", \"cube\", \n",
    "                       \"garnishing\", \"tablespo\", \"drop\", \"shaving\", \"drained\", \"grated\", \"gram\", \"soaked\", \"roughly\", \"blanched\",\n",
    "                        \"scraped\", \"ice\", \"large\", \"halved\", \"washed\", \"seeded\", \"in\", \"deveined\", \"diced\", \"slivered\", \"shelled\",\n",
    "                       \"quartered\", \"bulb\", \"overnight\", \"on\", \"pearl\", \"desiccated\", \"the\", \"at\", \"from\", \"cup\", \"one\", \"two\", \n",
    "                       \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"plenty\", \"of\", \"\", \" \"]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    if isinstance(ingreds, list):\n",
    "        items = ingreds\n",
    "    else:\n",
    "        items = ast.literal_eval(ingreds)\n",
    "        \n",
    "    items = [unidecode.unidecode(word) for word in items]\n",
    "    items = [word.replace(\"(\", \"\") for word in items]\n",
    "    items = [word.replace(\")\", \"\") for word in items]\n",
    "    items = [word.replace(\"/\", \" \") for word in items]\n",
    "    items = [word.replace(\"-\", \" \") for word in items]\n",
    "    items = [word.replace(\"+\", \" \") for word in items]\n",
    "    items = [word.replace(\"?\", \" \") for word in items]\n",
    "    # remove punctuations\n",
    "    # check against tuitiorial \n",
    "    items = [word.split(\",\")[0] for word in items]\n",
    "    items = [re.split(r'(\\d+)', word)[0] for word in items]\n",
    "    items = [word.lower() for word in items]\n",
    "    items = [word.split(\" to\")[0] for word in items]\n",
    "    items = [word.split(\"for\")[0] for word in items]\n",
    "    items = [word.split(\"cut\")[0] for word in items]\n",
    "    items = [word.split(\" with\")[0] for word in items]\n",
    "    items = [word.replace(\"  \", \" \") for word in items]\n",
    "    items = [word for word in items if word.find(\"oil\") == -1]\n",
    "    items = [word for word in items if word.find(\"salt\") == -1]\n",
    "    items = [word for word in items if word.find(\"butter\") == -1]\n",
    "    items = [word for word in items if word.find(\"sugar\") == -1]\n",
    "    items = [word for word in items if word.find(\"lemon\") == -1]\n",
    "    # lemmatizing words\n",
    "    items_lemmatized = []\n",
    "    for i in range(0, len(items)):\n",
    "        to_lemma = items[i].split(' ')\n",
    "        word_lemmatized = [lemmatizer.lemmatize(word) for word in to_lemma]\n",
    "        word_cleaned = ' '.join([word for word in word_lemmatized if word not in words_to_remove])\n",
    "        items_lemmatized.append(word_cleaned.lstrip().rstrip())\n",
    "    items_lemmatized = [word for word in items_lemmatized if word != '']\n",
    "#     items = listToString(items)\n",
    "#     items = [lemmatizer.lemmatize(word) for word in list(items.split(\" \"))]\n",
    "#     items = [word for word in items if word not in words_to_remove]\n",
    "#     items = [word for word in items if word not in stop_words]\n",
    "#     lammetize and change plurals from words_to_remove\n",
    "#     items = listToString(items)\n",
    "    return items_lemmatized   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "887e735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('df_scrubbed.csv')\n",
    "    df = df.dropna()\n",
    "    df['ingredients_parsed'] = df['ingredients'].apply(lambda x: ingredient_cleaner(x))\n",
    "    df = df.iloc[: , 1:]\n",
    "\n",
    "    df.to_csv('df_parsed_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adef81c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_parsed_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
